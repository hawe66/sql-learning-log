{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfhuy2bTThAKmSBeHP9fqU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0bab7232850140b49a6d527e0d5e74e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74ba19591aed418d921554f479c81f5d",
              "IPY_MODEL_10b2d590dd1a4150b4026cd35fa1a30c",
              "IPY_MODEL_7c16be34b10f4134af3c8e146e10da86"
            ],
            "layout": "IPY_MODEL_c217b1e5513344dd8a3aa8771482d279"
          }
        },
        "74ba19591aed418d921554f479c81f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75bf94f58ab14b77ba493ca1670953ac",
            "placeholder": "​",
            "style": "IPY_MODEL_b2c5aca3998846a981fc737a29f9e1c8",
            "value": "Map: 100%"
          }
        },
        "10b2d590dd1a4150b4026cd35fa1a30c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de8ff66d0f2f488d8b1b09d6ec0dbc93",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71e152691eae4d2298a76cfb04d10a96",
            "value": 5000
          }
        },
        "7c16be34b10f4134af3c8e146e10da86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_453d285247a443568a359b8cbe883f7d",
            "placeholder": "​",
            "style": "IPY_MODEL_6f8cd1b630de464b8d84a46061c95b8c",
            "value": " 5000/5000 [00:36&lt;00:00, 151.46 examples/s]"
          }
        },
        "c217b1e5513344dd8a3aa8771482d279": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75bf94f58ab14b77ba493ca1670953ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2c5aca3998846a981fc737a29f9e1c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de8ff66d0f2f488d8b1b09d6ec0dbc93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71e152691eae4d2298a76cfb04d10a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "453d285247a443568a359b8cbe883f7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f8cd1b630de464b8d84a46061c95b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cc65c5b5d274c5fbd92f06d7725cddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49ffb2cb44ee4d2b931f51c73b12cdf9",
              "IPY_MODEL_8c1293612cfc454781f60b9dcc948bc2",
              "IPY_MODEL_1ebca7a2dc1344a48ab51250a26c57b6"
            ],
            "layout": "IPY_MODEL_fbc8bfe788bb4322b66b089c2c674d75"
          }
        },
        "49ffb2cb44ee4d2b931f51c73b12cdf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7818ba6a181c435c922d8f1b50b7ac29",
            "placeholder": "​",
            "style": "IPY_MODEL_e0e7897432f24f3380e22ab1ecf14371",
            "value": "config.json: 100%"
          }
        },
        "8c1293612cfc454781f60b9dcc948bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f2faa39df274d98a4e590734b1a0c4a",
            "max": 718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b5569997bf54830a1d7f3f428895ef0",
            "value": 718
          }
        },
        "1ebca7a2dc1344a48ab51250a26c57b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14281b5aba0e4aa49b45bf206ec69fb5",
            "placeholder": "​",
            "style": "IPY_MODEL_022b95aec7c342ab85c1ed3afb515e69",
            "value": " 718/718 [00:00&lt;00:00, 63.6kB/s]"
          }
        },
        "fbc8bfe788bb4322b66b089c2c674d75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7818ba6a181c435c922d8f1b50b7ac29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e7897432f24f3380e22ab1ecf14371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f2faa39df274d98a4e590734b1a0c4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b5569997bf54830a1d7f3f428895ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14281b5aba0e4aa49b45bf206ec69fb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "022b95aec7c342ab85c1ed3afb515e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc4f9c666b6a4b3f906f479ebc4c1500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42ad3a872fb246e2a037960ec43c5876",
              "IPY_MODEL_e5e494cca8f744e0bf09744308bc5b3d",
              "IPY_MODEL_e6f5e900a1bd44bdb71db75b22743e04"
            ],
            "layout": "IPY_MODEL_1eaafb8a70c2429e8b713488b1731d60"
          }
        },
        "42ad3a872fb246e2a037960ec43c5876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfb28e0a753440d5bcdbdf10e7fb6a26",
            "placeholder": "​",
            "style": "IPY_MODEL_b95e9c5e8118450da2fa62eeb49f1f06",
            "value": "model.safetensors: 100%"
          }
        },
        "e5e494cca8f744e0bf09744308bc5b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62a8df13674b49cc9e9094d4c637c471",
            "max": 1519984962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cd52a3a1a3e4871969e6dcd4023ba83",
            "value": 1519984962
          }
        },
        "e6f5e900a1bd44bdb71db75b22743e04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_201be338c0074213a7954a183f9a6122",
            "placeholder": "​",
            "style": "IPY_MODEL_4d363ca72f3d489aacc765914fe23f32",
            "value": " 1.52G/1.52G [00:27&lt;00:00, 63.6MB/s]"
          }
        },
        "1eaafb8a70c2429e8b713488b1731d60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb28e0a753440d5bcdbdf10e7fb6a26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b95e9c5e8118450da2fa62eeb49f1f06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62a8df13674b49cc9e9094d4c637c471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cd52a3a1a3e4871969e6dcd4023ba83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "201be338c0074213a7954a183f9a6122": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d363ca72f3d489aacc765914fe23f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10ee29480be74f518583b12ddf10d4ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53d85252e71f4d83a8c160944c8c0956",
              "IPY_MODEL_028eb07128ec433e9ffbe7adcb36f849",
              "IPY_MODEL_8a2aa2d047c2486b8483d9324317fea3"
            ],
            "layout": "IPY_MODEL_ff9f0b1ef638495eb9b8a1d553408d7d"
          }
        },
        "53d85252e71f4d83a8c160944c8c0956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_795af624b12c497893d9ba9b2e4c5da7",
            "placeholder": "​",
            "style": "IPY_MODEL_5353591bc6f542b7acfeed78d060ef9d",
            "value": "generation_config.json: 100%"
          }
        },
        "028eb07128ec433e9ffbe7adcb36f849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d0da629109c4f9497c6812615820424",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f08d6bff9f434d8ea8ac32901e081aca",
            "value": 124
          }
        },
        "8a2aa2d047c2486b8483d9324317fea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f05db9b02df4c84b2c8505a1757cfac",
            "placeholder": "​",
            "style": "IPY_MODEL_d0dc5a588e3047fa9d8bc9e2f8b20035",
            "value": " 124/124 [00:00&lt;00:00, 11.4kB/s]"
          }
        },
        "ff9f0b1ef638495eb9b8a1d553408d7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "795af624b12c497893d9ba9b2e4c5da7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5353591bc6f542b7acfeed78d060ef9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d0da629109c4f9497c6812615820424": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f08d6bff9f434d8ea8ac32901e081aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f05db9b02df4c84b2c8505a1757cfac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0dc5a588e3047fa9d8bc9e2f8b20035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawe66/sql-learning-log/blob/main/slm_lora_toy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rQbbsYW_ioW9",
        "outputId": "14c89684-ac81-4b8a-811d-76aac4065c29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: sagemaker in /usr/local/lib/python3.12/dist-packages (2.251.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.12/dist-packages (1.40.25)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: attrs<26,>=24 in /usr/local/lib/python3.12/dist-packages (from sagemaker) (25.3.0)\n",
            "Requirement already satisfied: cloudpickle>=2.2.1 in /usr/local/lib/python3.12/dist-packages (from sagemaker) (3.1.1)\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.12/dist-packages (from sagemaker) (7.1.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from sagemaker) (0.116.1)\n",
            "Requirement already satisfied: google-pasta in /usr/local/lib/python3.12/dist-packages (from sagemaker) (0.2.0)\n",
            "Requirement already satisfied: graphene<4,>=3 in /usr/local/lib/python3.12/dist-packages (from sagemaker) (3.4.3)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from sagemaker) (6.11.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from sagemaker) (4.25.1)\n",
            "Requirement already satisfied: omegaconf<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from sagemaker) (2.3.0)\n",
            "Requirement already satisfied: pathos in /usr/local/lib/python3.12/dist-packages (from sagemaker) (0.3.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from sagemaker) (4.4.0)\n",
            "Requirement already satisfied: protobuf<6.32,>=3.12 in /usr/local/lib/python3.12/dist-packages (from sagemaker) (5.29.5)\n",
            "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from sagemaker) (1.0.57)\n",
            "Requirement already satisfied: schema in /usr/local/lib/python3.12/dist-packages (from sagemaker) (0.7.7)\n",
            "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /usr/local/lib/python3.12/dist-packages (from sagemaker) (1.0.1)\n",
            "Requirement already satisfied: tblib<4,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from sagemaker) (3.1.0)\n",
            "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /usr/local/lib/python3.12/dist-packages (from sagemaker) (2.5.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (from sagemaker) (0.35.0)\n",
            "Requirement already satisfied: botocore<1.41.0,>=1.40.25 in /usr/local/lib/python3.12/dist-packages (from boto3) (1.40.25)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from boto3) (0.13.1)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4,>=3->sagemaker) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4,>=3->sagemaker) (3.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.23.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf<3,>=2.2->sagemaker) (4.9.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.11.7)\n",
            "Requirement already satisfied: rich<15.0.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (13.9.4)\n",
            "Requirement already satisfied: mock<5.0,>4.0 in /usr/local/lib/python3.12/dist-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (4.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->sagemaker) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->sagemaker) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->sagemaker) (0.27.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.19.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi->sagemaker) (0.47.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi->sagemaker) (4.10.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi->sagemaker) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: ppft>=1.7.6.8 in /usr/local/lib/python3.12/dist-packages (from pathos->sagemaker) (1.7.7)\n",
            "Requirement already satisfied: pox>=0.3.4 in /usr/local/lib/python3.12/dist-packages (from pathos->sagemaker) (0.3.6)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn->sagemaker) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn->sagemaker) (0.16.0)\n",
            "Collecting git+https://github.com/TimDettmers/bitsandbytes.git\n",
            "  Cloning https://github.com/TimDettmers/bitsandbytes.git to /tmp/pip-req-build-w1gt2rm4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/TimDettmers/bitsandbytes.git /tmp/pip-req-build-w1gt2rm4\n",
            "  Resolved https://github.com/TimDettmers/bitsandbytes.git to commit 39dd8471c1c0677001d0d20ba2218b14bf18fd00\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.48.0.dev0) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.48.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.48.0.dev0) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes==0.48.0.dev0) (3.0.2)\n",
            "Building wheels for collected packages: bitsandbytes\n",
            "  Building wheel for bitsandbytes (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bitsandbytes: filename=bitsandbytes-0.48.0.dev0-cp312-cp312-linux_x86_64.whl size=121772 sha256=3e2e2ae5f9c326a6670e514dec42e2485af308ad55269ead4ee3c92689d5b261\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0p8v00x3/wheels/c8/ab/0c/384ba13843c21eb3696c157f9ae2ba4b64b330d31973379f2b\n",
            "Successfully built bitsandbytes\n",
            "Installing collected packages: bitsandbytes\n",
            "  Attempting uninstall: bitsandbytes\n",
            "    Found existing installation: bitsandbytes 0.47.0\n",
            "    Uninstalling bitsandbytes-0.47.0:\n",
            "      Successfully uninstalled bitsandbytes-0.47.0\n",
            "Successfully installed bitsandbytes-0.48.0.dev0\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "\u001b[33m  DEPRECATION: Building 'rouge_score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge_score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=1c133b452bfc10ddd497406ea2d5d68d49fd01ce163fc3460a1c8303dc4d5d46\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "# 최신 pip\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# 핵심 라이브러리\n",
        "!pip install torch transformers datasets accelerate evaluate matplotlib sagemaker boto3\n",
        "\n",
        "# LoRA / QLoRA 관련\n",
        "!pip install -q --no-deps xformers trl peft accelerate\n",
        "!pip install git+https://github.com/TimDettmers/bitsandbytes.git\n",
        "\n",
        "# 평가 지표용 (ROUGE 등)\n",
        "!pip install rouge_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset and model"
      ],
      "metadata": {
        "id": "lF6YBB3wkv2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# [🔁 모델 변경 가능] -- GPT 대신 T5, BART 등으로 교체 시 여기를 바꾸세요.\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# GPT류에는 pad_token이 없으므로 eos를 재사용\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "cnn_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n"
      ],
      "metadata": {
        "id": "TbPpJadljQvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text\n",
        "def distilgpt2_generate_text(text, model, max_new_tokens=128):\n",
        "    input = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
        "    output = model.generate(\n",
        "        input,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=1.1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "rc3pXd6SkptV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing via example\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4O0ATR0clQwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "highlight = cnn_dataset[\"train\"][10]['highlights']\n",
        "print(\"Reference Summary:\")\n",
        "print(highlight)\n",
        "sentence = (\n",
        "    \"The city of San Francisco has introduced a new pilot program to reduce traffic congestion. \"\n",
        "    \"Officials say the plan will involve rerouting certain streets and implementing new traffic signals.\"\n",
        ")\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Tokenized:\")\n",
        "print(tokens)\n",
        "print(\"Token IDs:\")\n",
        "print(token_ids)\n",
        "\n",
        "# 테스트 문장 생성\n",
        "sample_sentence = (\n",
        "    \"The city of San Francisco has introduced a new pilot program to reduce traffic congestion. \"\n",
        "    \"Officials say the plan will involve rerouting certain streets and implementing new traffic signals.\"\n",
        ")\n",
        "\n",
        "# 텍스트 생성 함수\n",
        "def generate_text(text, model):\n",
        "    input = tokenizer.encode(text, return_tensors=\"pt\").to(model.device)\n",
        "    output = model.generate(\n",
        "        input,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        temperature=0.8,\n",
        "        repetition_penalty=1.1,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        attention_mask=(input != tokenizer.pad_token_id).long()\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(generate_text(sample_sentence, model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-qNgG0FlIG4",
        "outputId": "fef6f123-6a27-4be4-a096-68693594ee74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference Summary:\n",
            "President Bush to address the Veterans of Foreign Wars on Wednesday .\n",
            "Bush to say that withdrawing from Vietnam emboldened today's terrorists .\n",
            "Speech will be latest White House attempt to try to reframe the debate over Iraq .\n",
            "Tokenized:\n",
            "['The', 'Ġcity', 'Ġof', 'ĠSan', 'ĠFrancisco', 'Ġhas', 'Ġintroduced', 'Ġa', 'Ġnew', 'Ġpilot', 'Ġprogram', 'Ġto', 'Ġreduce', 'Ġtraffic', 'Ġcongestion', '.', 'ĠOfficials', 'Ġsay', 'Ġthe', 'Ġplan', 'Ġwill', 'Ġinvolve', 'Ġre', 'r', 'outing', 'Ġcertain', 'Ġstreets', 'Ġand', 'Ġimplementing', 'Ġnew', 'Ġtraffic', 'Ġsignals', '.']\n",
            "Token IDs:\n",
            "[464, 1748, 286, 2986, 6033, 468, 5495, 257, 649, 8022, 1430, 284, 4646, 4979, 28014, 13, 28244, 910, 262, 1410, 481, 6211, 302, 81, 13660, 1728, 6483, 290, 15427, 649, 4979, 10425, 13]\n",
            "The city of San Francisco has introduced a new pilot program to reduce traffic congestion.\n",
            "This year, the City Council approved two measures that would have eliminated one major transit lane on West Side streets: 1) eliminate drivers from public transportation by creating an extra 60 lanes between 7th and 10th Streets in three blocks (the other part will be built later this year). The change was originally announced as a \"no-passed\" initiative for bike commuters who wish they could use their bicycle or car at designated intersections after completing all four phases. But it's now being implemented through its\n",
            "tensor([[  464,  1748,   286,  2986,  6033,   468,  5495,   257,   649,  8022,\n",
            "          1430,   284,  4646,  4979, 28014,    13,   198,  1212,   614,    11,\n",
            "           262,  2254,  4281,  6325,   734,  5260,   326,   561,   423, 15254,\n",
            "           530,  1688, 11168, 11193,   319,  2688, 12075,  6483,    25,   352,\n",
            "             8, 11005,  6643,   422,  1171,  9358,   416,  4441,   281,  3131,\n",
            "          3126, 15296,  1022,   767,   400,   290,   838,   400, 27262,   287,\n",
            "          1115,  7021,   357,  1169,   584,   636,   481,   307,  3170,  1568,\n",
            "           428,   614,   737,   383,  1487,   373,  6198,  3414,   355,   257,\n",
            "           366,  3919,    12,  6603,   276,     1, 10219,   329,  7161, 36799,\n",
            "           508,  4601,   484,   714,   779,   511, 17026,   393,  1097,   379,\n",
            "         11032, 42085,   706, 14339,   477,  1440, 21164,    13,   887,   340,\n",
            "           338,   783,   852,  9177,   832,   663]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4563131"
      },
      "source": [
        "# Task\n",
        "Generate Python code to perform full fine-tuning and LoRA on DistilGPT-2 using a sampled subset of the cnn_dailymail dataset for a summarization task, and include code to measure training time, VRAM usage, and the number of tuned parameters for comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f6ae83c"
      },
      "source": [
        "## 데이터 샘플링 및 전처리\n",
        "\n",
        "### Subtask:\n",
        "cnn_dailymail 데이터셋의 일부를 샘플링하고 모델 학습에 필요한 형식으로 전처리합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c68c091"
      },
      "source": [
        "**Reasoning**:\n",
        "Sample the dataset, combine the article and highlights, tokenize the combined text, and format it for model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0bab7232850140b49a6d527e0d5e74e6",
            "74ba19591aed418d921554f479c81f5d",
            "10b2d590dd1a4150b4026cd35fa1a30c",
            "7c16be34b10f4134af3c8e146e10da86",
            "c217b1e5513344dd8a3aa8771482d279",
            "75bf94f58ab14b77ba493ca1670953ac",
            "b2c5aca3998846a981fc737a29f9e1c8",
            "de8ff66d0f2f488d8b1b09d6ec0dbc93",
            "71e152691eae4d2298a76cfb04d10a96",
            "453d285247a443568a359b8cbe883f7d",
            "6f8cd1b630de464b8d84a46061c95b8c"
          ]
        },
        "id": "abd71bfa",
        "outputId": "6fcdcce8-b5a1-4a19-f219-b222db37751a"
      },
      "source": [
        "import torch\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"{a} [SEP] {h}\" for a, h in zip(examples[\"article\"], examples[\"highlights\"])]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "    return model_inputs\n",
        "\n",
        "# [🔁 샘플 수 변경 가능]\n",
        "train_sample_size = 5000\n",
        "eval_sample_size = 500\n",
        "sampled_train_dataset = cnn_dataset[\"train\"].select(range(train_sample_size))\n",
        "sampled_test_dataset = cnn_dataset[\"test\"].select(range(eval_sample_size))\n",
        "\n",
        "# 토크나이즈\n",
        "tokenized_train_dataset = sampled_train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test_dataset = sampled_test_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "tokenized_test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bab7232850140b49a6d527e0d5e74e6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0f8f87d"
      },
      "source": [
        "## Full fine-tuning 모델 학습\n",
        "\n",
        "### Subtask:\n",
        "샘플링된 데이터셋을 사용하여 DistilGPT-2 모델을 Full Fine-tuning합니다. 학습 시간과 VRAM 사용량을 기록합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6d6b5d3"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary classes, set the device, create TrainingArguments, initialize the Trainer, start training, and record training time and VRAM usage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8313486d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `evaluation_strategy` is not a valid argument for `TrainingArguments`. Based on the traceback, I will remove the `evaluation_strategy` argument and replace it with `eval_strategy` as per the transformers library documentation. I will also update `logging_steps` to `logging_steps=50` and `warmup_steps` to `warmup_ratio=0.06` as suggested by the documentation to improve training stability. I will also adjust `save_strategy` and `save_steps` to save the model at the end of each epoch and at intervals of 500 steps to ensure the model is saved during training. I will also set `push_to_hub=False` as we are not pushing the model to the Hugging Face Hub. Finally, I will set `load_best_model_at_end=True` and `metric_for_best_model=\"loss\"` to load the best model at the end of training based on the loss metric.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "SRo4-Jk7mbpD",
        "outputId": "b984749e-7dbd-4a79-ab43-5015128b5c51"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# [🔁 학습 조건 변경 가능]\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_baseline\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,                      # 🔁 기존보다 큰 학습률\n",
        "    num_train_epochs=5,                      # 🔁 에폭 수 증가\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,                        # 🔁 warmup step 명시\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs_baseline\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"\\nTraining time: {end_time - start_time:.2f} seconds\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"Peak VRAM usage: {torch.cuda.max_memory_allocated(device)/1024**2:.2f} MB\")\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-884804771.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5678' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5678/6250 39:25 < 03:58, 2.40 it/s, Epoch 4.54/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.112000</td>\n",
              "      <td>2.961987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.921900</td>\n",
              "      <td>2.973611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.892700</td>\n",
              "      <td>2.982705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.782400</td>\n",
              "      <td>2.988582</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6250/6250 44:13, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.112000</td>\n",
              "      <td>2.961987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.921900</td>\n",
              "      <td>2.973611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.892700</td>\n",
              "      <td>2.982705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.782400</td>\n",
              "      <td>2.988582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.769300</td>\n",
              "      <td>2.994571</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training time: 2654.71 seconds\n",
            "Peak VRAM usage: 3284.71 MB\n",
            "Number of parameters: 81912576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "2mk5JqUYm7rK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9ffa11-21c2-4319-9e0c-59c6c41f1c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aebbc658"
      },
      "source": [
        "# Task\n",
        "Continue the experiment by implementing and training DistilGPT-2 with LoRA on the sampled cnn_dailymail dataset, then evaluate and compare it with the full fine-tuned DistilGPT-2 model based on the provided requirements. If time and resources allow, proceed with the optional GPT-2 Medium experiments with LoRA and QLoRA. Finally, summarize all findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cac7a30"
      },
      "source": [
        "## Implement lora on distilgpt-2\n",
        "\n",
        "### Subtask:\n",
        "Apply LoRA to the DistilGPT-2 model using the sampled cnn_dailymail dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "945237c8",
        "outputId": "03c106dd-3a88-4d6d-e119-61d62b69afd0"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# LoRA 구성\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# 모델에 LoRA 적용\n",
        "pretrained_model = AutoModelForCausalLM.from_pretrained(checkpoint_path, device_map=\"auto\")\n",
        "lora_model = get_peft_model(pretrained_model, lora_config)\n",
        "lora_model.print_trainable_parameters()\n",
        "lora_model.to(device)\n",
        "\n",
        "# VRAM 초기 사용량 기록\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.reset_peak_memory_stats(device)\n",
        "    initial_lora_vram = torch.cuda.memory_allocated(device) / 1024**2\n",
        "    print(f\"Initial VRAM usage (LoRA model): {initial_lora_vram:.2f} MB\")\n",
        "\n",
        "# TrainingArguments 설정\n",
        "lora_training_args = TrainingArguments(\n",
        "    output_dir=\"./results_lora\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,                      # 🔁 기존보다 큰 학습률\n",
        "    num_train_epochs=5,                      # 🔁 에폭 수 증가\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,                        # 🔁 warmup step 명시\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs_lora\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer 초기화\n",
        "lora_trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=lora_training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# 학습 시간 측정 시작\n",
        "start_time_lora = time.time()\n",
        "\n",
        "# ✅ resume 지원: 필요 시 체크포인트 지정 가능\n",
        "# 예: resume_from_checkpoint=\"./results_lora/checkpoint-500\"\n",
        "# lora_trainer.train(resume_from_checkpoint=True)\n",
        "lora_trainer.train()\n",
        "\n",
        "# 학습 종료 후 시간 측정\n",
        "end_time_lora = time.time()\n",
        "training_time_lora = end_time_lora - start_time_lora\n",
        "print(f\"\\nTraining time (LoRA): {training_time_lora:.2f} seconds\")\n",
        "\n",
        "# 최대 VRAM 사용량 기록\n",
        "if device.type == 'cuda':\n",
        "    peak_lora_vram = torch.cuda.max_memory_allocated(device) / 1024**2\n",
        "    print(f\"Peak VRAM usage during training (LoRA): {peak_lora_vram:.2f} MB\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "trainable params: 405,504 || all params: 82,318,080 || trainable%: 0.4926\n",
            "Initial VRAM usage (LoRA model): 1290.24 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3874485968.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  lora_trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6250/6250 30:44, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.305800</td>\n",
              "      <td>3.062060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.153400</td>\n",
              "      <td>2.977643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.188300</td>\n",
              "      <td>2.962579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.101500</td>\n",
              "      <td>2.955986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.143700</td>\n",
              "      <td>2.953976</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training time (LoRA): 1845.09 seconds\n",
            "Peak VRAM usage during training (LoRA): 3659.93 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2e668e9"
      },
      "source": [
        "## Evaluate distilgpt-2 models\n",
        "\n",
        "### Subtask:\n",
        "Calculate evaluation metrics (Perplexity, ROUGE) for both the full fine-tuned and LoRA models on the test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "# [NEW] Load checkpointed baseline model\n",
        "checkpoint_path = \"./results_baseline/checkpoint-6250\"\n",
        "baseline_model = AutoModelForCausalLM.from_pretrained(checkpoint_path, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "generation_config = GenerationConfig.from_pretrained(checkpoint_path)\n",
        "baseline_model.generation_config = generation_config\n"
      ],
      "metadata": {
        "id": "ldzEq1-DbXTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jmNBl7svQ6u",
        "outputId": "4e25b7d3-d052-43bd-acef-c511f5207dd0"
      },
      "source": [
        "import time\n",
        "import gc\n",
        "import torch\n",
        "import evaluate\n",
        "\n",
        "# Load evaluation metrics\n",
        "perplexity_metric = evaluate.load(\"perplexity\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "# Set tokenizer padding side for generation\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move models to device\n",
        "baseline_model.to(device)\n",
        "lora_model.to(device)\n",
        "\n",
        "# Define evaluation function\n",
        "def compute_metrics(model, dataset, tokenizer, original_dataset, use_base_model=False, max_eval=100):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for i in range(min(len(dataset), max_eval)):\n",
        "        print(f\"Processing example {i+1}/{min(len(dataset), max_eval)}\")\n",
        "\n",
        "        try:\n",
        "            # Get example\n",
        "            example = dataset[i]\n",
        "            reference = original_dataset[i][\"highlights\"]\n",
        "\n",
        "            input_ids = example[\"input_ids\"].unsqueeze(0).to(model.device)\n",
        "            attention_mask = example[\"attention_mask\"].unsqueeze(0).to(model.device)\n",
        "\n",
        "            # Clean up memory before generate\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                gen_model = getattr(model, \"base_model\", model) if use_base_model else model\n",
        "\n",
        "                generated = gen_model.generate(\n",
        "                    input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    max_new_tokens=128,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            # Decode prediction\n",
        "            decoded = tokenizer.batch_decode(generated[:, input_ids.shape[1]:], skip_special_tokens=True)\n",
        "            predictions.extend(decoded)\n",
        "            references.append(reference)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] Example {i+1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Compute ROUGE\n",
        "    rouge_scores = rouge_metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "    return rouge_scores\n",
        "\n",
        "# Run evaluation\n",
        "print(\"🧪 Evaluating Full Fine-tuned Model:\")\n",
        "baseline_rouge = compute_metrics(baseline_model, tokenized_test_dataset, tokenizer, sampled_test_dataset)\n",
        "print(f\"📊 ROUGE (Full): {baseline_rouge}\")\n",
        "\n",
        "print(\"\\n🧪 Evaluating LoRA Fine-tuned Model:\")\n",
        "lora_rouge = compute_metrics(lora_model, tokenized_test_dataset, tokenizer, sampled_test_dataset, use_base_model=True)\n",
        "print(f\"📊 ROUGE (LoRA): {lora_rouge}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Evaluating Full Fine-tuned Model:\n",
            "Processing example 1/100\n",
            "Processing example 2/100\n",
            "Processing example 3/100\n",
            "Processing example 4/100\n",
            "Processing example 5/100\n",
            "Processing example 6/100\n",
            "Processing example 7/100\n",
            "Processing example 8/100\n",
            "Processing example 9/100\n",
            "Processing example 10/100\n",
            "Processing example 11/100\n",
            "Processing example 12/100\n",
            "Processing example 13/100\n",
            "Processing example 14/100\n",
            "Processing example 15/100\n",
            "Processing example 16/100\n",
            "Processing example 17/100\n",
            "Processing example 18/100\n",
            "Processing example 19/100\n",
            "Processing example 20/100\n",
            "Processing example 21/100\n",
            "Processing example 22/100\n",
            "Processing example 23/100\n",
            "Processing example 24/100\n",
            "Processing example 25/100\n",
            "Processing example 26/100\n",
            "Processing example 27/100\n",
            "Processing example 28/100\n",
            "Processing example 29/100\n",
            "Processing example 30/100\n",
            "Processing example 31/100\n",
            "Processing example 32/100\n",
            "Processing example 33/100\n",
            "Processing example 34/100\n",
            "Processing example 35/100\n",
            "Processing example 36/100\n",
            "Processing example 37/100\n",
            "Processing example 38/100\n",
            "Processing example 39/100\n",
            "Processing example 40/100\n",
            "Processing example 41/100\n",
            "Processing example 42/100\n",
            "Processing example 43/100\n",
            "Processing example 44/100\n",
            "Processing example 45/100\n",
            "Processing example 46/100\n",
            "Processing example 47/100\n",
            "Processing example 48/100\n",
            "Processing example 49/100\n",
            "Processing example 50/100\n",
            "Processing example 51/100\n",
            "Processing example 52/100\n",
            "Processing example 53/100\n",
            "Processing example 54/100\n",
            "Processing example 55/100\n",
            "Processing example 56/100\n",
            "Processing example 57/100\n",
            "Processing example 58/100\n",
            "Processing example 59/100\n",
            "Processing example 60/100\n",
            "Processing example 61/100\n",
            "Processing example 62/100\n",
            "Processing example 63/100\n",
            "Processing example 64/100\n",
            "Processing example 65/100\n",
            "Processing example 66/100\n",
            "Processing example 67/100\n",
            "Processing example 68/100\n",
            "Processing example 69/100\n",
            "Processing example 70/100\n",
            "Processing example 71/100\n",
            "Processing example 72/100\n",
            "Processing example 73/100\n",
            "Processing example 74/100\n",
            "Processing example 75/100\n",
            "Processing example 76/100\n",
            "Processing example 77/100\n",
            "Processing example 78/100\n",
            "Processing example 79/100\n",
            "Processing example 80/100\n",
            "Processing example 81/100\n",
            "Processing example 82/100\n",
            "Processing example 83/100\n",
            "Processing example 84/100\n",
            "Processing example 85/100\n",
            "Processing example 86/100\n",
            "Processing example 87/100\n",
            "Processing example 88/100\n",
            "Processing example 89/100\n",
            "Processing example 90/100\n",
            "Processing example 91/100\n",
            "Processing example 92/100\n",
            "Processing example 93/100\n",
            "Processing example 94/100\n",
            "Processing example 95/100\n",
            "Processing example 96/100\n",
            "Processing example 97/100\n",
            "Processing example 98/100\n",
            "Processing example 99/100\n",
            "Processing example 100/100\n",
            "📊 ROUGE (Full): {'rouge1': 0.12513820251981816, 'rouge2': 0.036925766438128865, 'rougeL': 0.10367485752402496, 'rougeLsum': 0.11084457458704254}\n",
            "\n",
            "🧪 Evaluating LoRA Fine-tuned Model:\n",
            "Processing example 1/100\n",
            "Processing example 2/100\n",
            "Processing example 3/100\n",
            "Processing example 4/100\n",
            "Processing example 5/100\n",
            "Processing example 6/100\n",
            "Processing example 7/100\n",
            "Processing example 8/100\n",
            "Processing example 9/100\n",
            "Processing example 10/100\n",
            "Processing example 11/100\n",
            "Processing example 12/100\n",
            "Processing example 13/100\n",
            "Processing example 14/100\n",
            "Processing example 15/100\n",
            "Processing example 16/100\n",
            "Processing example 17/100\n",
            "Processing example 18/100\n",
            "Processing example 19/100\n",
            "Processing example 20/100\n",
            "Processing example 21/100\n",
            "Processing example 22/100\n",
            "Processing example 23/100\n",
            "Processing example 24/100\n",
            "Processing example 25/100\n",
            "Processing example 26/100\n",
            "Processing example 27/100\n",
            "Processing example 28/100\n",
            "Processing example 29/100\n",
            "Processing example 30/100\n",
            "Processing example 31/100\n",
            "Processing example 32/100\n",
            "Processing example 33/100\n",
            "Processing example 34/100\n",
            "Processing example 35/100\n",
            "Processing example 36/100\n",
            "Processing example 37/100\n",
            "Processing example 38/100\n",
            "Processing example 39/100\n",
            "Processing example 40/100\n",
            "Processing example 41/100\n",
            "Processing example 42/100\n",
            "Processing example 43/100\n",
            "Processing example 44/100\n",
            "Processing example 45/100\n",
            "Processing example 46/100\n",
            "Processing example 47/100\n",
            "Processing example 48/100\n",
            "Processing example 49/100\n",
            "Processing example 50/100\n",
            "Processing example 51/100\n",
            "Processing example 52/100\n",
            "Processing example 53/100\n",
            "Processing example 54/100\n",
            "Processing example 55/100\n",
            "Processing example 56/100\n",
            "Processing example 57/100\n",
            "Processing example 58/100\n",
            "Processing example 59/100\n",
            "Processing example 60/100\n",
            "Processing example 61/100\n",
            "Processing example 62/100\n",
            "Processing example 63/100\n",
            "Processing example 64/100\n",
            "Processing example 65/100\n",
            "Processing example 66/100\n",
            "Processing example 67/100\n",
            "Processing example 68/100\n",
            "Processing example 69/100\n",
            "Processing example 70/100\n",
            "Processing example 71/100\n",
            "Processing example 72/100\n",
            "Processing example 73/100\n",
            "Processing example 74/100\n",
            "Processing example 75/100\n",
            "Processing example 76/100\n",
            "Processing example 77/100\n",
            "Processing example 78/100\n",
            "Processing example 79/100\n",
            "Processing example 80/100\n",
            "Processing example 81/100\n",
            "Processing example 82/100\n",
            "Processing example 83/100\n",
            "Processing example 84/100\n",
            "Processing example 85/100\n",
            "Processing example 86/100\n",
            "Processing example 87/100\n",
            "Processing example 88/100\n",
            "Processing example 89/100\n",
            "Processing example 90/100\n",
            "Processing example 91/100\n",
            "Processing example 92/100\n",
            "Processing example 93/100\n",
            "Processing example 94/100\n",
            "Processing example 95/100\n",
            "Processing example 96/100\n",
            "Processing example 97/100\n",
            "Processing example 98/100\n",
            "Processing example 99/100\n",
            "Processing example 100/100\n",
            "📊 ROUGE (LoRA): {'rouge1': 0.12962194291212187, 'rouge2': 0.03904711735689391, 'rougeL': 0.10910542141383855, 'rougeLsum': 0.11893577913098959}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4accfa31"
      },
      "source": [
        "## Generate text samples\n",
        "\n",
        "### Subtask:\n",
        "Generate text samples from both the full fine-tuned and LoRA models using the same prompt for qualitative comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50aea7ec"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a prompt and use the `distilgpt2_generate_text` function to generate text from both the baseline (full fine-tuned) and LoRA models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2da6a7c",
        "outputId": "fa439a2a-dad2-4b9d-e5ed-bcbc62ad5970"
      },
      "source": [
        "# Define a prompt for text generation\n",
        "prompt = \"Once upon a time...\"\n",
        "\n",
        "# Generate text using the full fine-tuned model\n",
        "print(\"Generating text from Full Fine-tuned Model:\")\n",
        "baseline_generated_text = distilgpt2_generate_text(prompt, baseline_model)\n",
        "print(\"Full Fine-tuned Model Output:\")\n",
        "print(baseline_generated_text)\n",
        "\n",
        "# Generate text using the LoRA model\n",
        "print(\"\\nGenerating text from LoRA Model:\")\n",
        "lora_generated_text = distilgpt2_generate_text(prompt, lora_model)\n",
        "print(\"LoRA Model Output:\")\n",
        "print(lora_generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating text from Full Fine-tuned Model:\n",
            "Full Fine-tuned Model Output:\n",
            "Once upon a time...\n",
            "\n",
            "I was in the basement, and had never seen some kind of room before. It looked\n",
            "\n",
            "Generating text from LoRA Model:\n",
            "LoRA Model Output:\n",
            "Once upon a time...\n",
            "To help you identify which type of item you are referring to, I present a list of 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46b02861"
      },
      "source": [
        "## Optional: GPT-2 Medium Experiments with LoRA and QLoRA\n",
        "\n",
        "Now, we can optionally explore applying LoRA and QLoRA to a larger model, GPT-2 Medium, to observe the differences in efficiency and performance compared to full fine-tuning. Please note that training GPT-2 Medium, even with LoRA or QLoRA, will require significant computational resources and may take a considerable amount of time in a Colab environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f0e6fc3"
      },
      "source": [
        "## Implement LoRA on GPT-2 Medium (Optional)\n",
        "\n",
        "### Subtask:\n",
        "Load the GPT-2 Medium model and apply LoRA to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97a327f0"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the GPT-2 Medium model, apply the LoRA configuration, and print the number of trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419,
          "referenced_widgets": [
            "5cc65c5b5d274c5fbd92f06d7725cddf",
            "49ffb2cb44ee4d2b931f51c73b12cdf9",
            "8c1293612cfc454781f60b9dcc948bc2",
            "1ebca7a2dc1344a48ab51250a26c57b6",
            "fbc8bfe788bb4322b66b089c2c674d75",
            "7818ba6a181c435c922d8f1b50b7ac29",
            "e0e7897432f24f3380e22ab1ecf14371",
            "2f2faa39df274d98a4e590734b1a0c4a",
            "0b5569997bf54830a1d7f3f428895ef0",
            "14281b5aba0e4aa49b45bf206ec69fb5",
            "022b95aec7c342ab85c1ed3afb515e69",
            "bc4f9c666b6a4b3f906f479ebc4c1500",
            "42ad3a872fb246e2a037960ec43c5876",
            "e5e494cca8f744e0bf09744308bc5b3d",
            "e6f5e900a1bd44bdb71db75b22743e04",
            "1eaafb8a70c2429e8b713488b1731d60",
            "cfb28e0a753440d5bcdbdf10e7fb6a26",
            "b95e9c5e8118450da2fa62eeb49f1f06",
            "62a8df13674b49cc9e9094d4c637c471",
            "1cd52a3a1a3e4871969e6dcd4023ba83",
            "201be338c0074213a7954a183f9a6122",
            "4d363ca72f3d489aacc765914fe23f32",
            "10ee29480be74f518583b12ddf10d4ae",
            "53d85252e71f4d83a8c160944c8c0956",
            "028eb07128ec433e9ffbe7adcb36f849",
            "8a2aa2d047c2486b8483d9324317fea3",
            "ff9f0b1ef638495eb9b8a1d553408d7d",
            "795af624b12c497893d9ba9b2e4c5da7",
            "5353591bc6f542b7acfeed78d060ef9d",
            "8d0da629109c4f9497c6812615820424",
            "f08d6bff9f434d8ea8ac32901e081aca",
            "8f05db9b02df4c84b2c8505a1757cfac",
            "d0dc5a588e3047fa9d8bc9e2f8b20035"
          ]
        },
        "id": "38c24af6",
        "outputId": "26f455ff-e6b6-4462-f445-6327f0144fba"
      },
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Load the GPT-2 Medium model\n",
        "print(\"Loading GPT-2 Medium model...\")\n",
        "gpt2_medium_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
        "gpt2_medium_model.to(device)\n",
        "print(\"GPT-2 Medium model loaded.\")\n",
        "\n",
        "# Define LoRA configuration (using the same config as for DistilGPT-2 for consistency)\n",
        "# We can adjust these parameters later if needed for performance on the larger model.\n",
        "lora_config_medium = LoraConfig(\n",
        "    r=8,  # LoRA attention dimension\n",
        "    lora_alpha=16,  # Alpha parameter for LoRA scaling\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],  # Modules to apply LoRA to\n",
        "    lora_dropout=0.05,  # Dropout probability for LoRA layers\n",
        "    bias=\"none\",  # Bias type\n",
        "    task_type=\"CAUSAL_LM\",  # Task type for causal language modeling\n",
        ")\n",
        "\n",
        "# Apply LoRA to the GPT-2 Medium model\n",
        "print(\"Applying LoRA to GPT-2 Medium model...\")\n",
        "lora_gpt2_medium_model = get_peft_model(gpt2_medium_model, lora_config_medium)\n",
        "print(\"LoRA applied to GPT-2 Medium model.\")\n",
        "\n",
        "# Print the number of trainable parameters in the LoRA GPT-2 Medium model\n",
        "print(\"\\nLoRA GPT-2 Medium model trainable parameters:\")\n",
        "lora_gpt2_medium_tuned_params = lora_gpt2_medium_model.print_trainable_parameters()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading GPT-2 Medium model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cc65c5b5d274c5fbd92f06d7725cddf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc4f9c666b6a4b3f906f479ebc4c1500"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10ee29480be74f518583b12ddf10d4ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 Medium model loaded.\n",
            "Applying LoRA to GPT-2 Medium model...\n",
            "LoRA applied to GPT-2 Medium model.\n",
            "\n",
            "LoRA GPT-2 Medium model trainable parameters:\n",
            "trainable params: 2,162,688 || all params: 356,985,856 || trainable%: 0.6058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b41ad593"
      },
      "source": [
        "## Implement QLoRA on GPT-2 Medium (Optional)\n",
        "\n",
        "### Subtask:\n",
        "Apply QLoRA to the GPT-2 Medium model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3c27da2"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the GPT-2 Medium model (if not already loaded) and apply the QLoRA configuration. Print the number of trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32bc0ace",
        "outputId": "ef66394c-10ce-4863-a818-8bf4d474f65d"
      },
      "source": [
        "# === 런타임 재시작 후 QLoRA 코드 실행 ===\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 4-bit 양자화 로딩\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "# 메모리 정리\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "import gc; gc.collect()\n",
        "\n",
        "print(\"Loading GPT-2 Medium with 4-bit quantization...\")\n",
        "gpt2_medium_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt2-medium\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"Quantized model loaded.\")\n",
        "\n",
        "# QLoRA 준비\n",
        "gpt2_medium_model = prepare_model_for_kbit_training(gpt2_medium_model)\n",
        "print(\"Model prepared for k-bit training.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading GPT-2 Medium with 4-bit quantization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:bitsandbytes.cextension:The 8-bit optimizer is not available on your device, only available on CUDA for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized model loaded.\n",
            "Model prepared for k-bit training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b03d8174"
      },
      "source": [
        "# Task Conclusion\n",
        "\n",
        "We have successfully completed the following parts of the task:\n",
        "\n",
        "*   Loading the dataset and model (DistilGPT-2).\n",
        "*   Sampling and preprocessing the dataset for training.\n",
        "*   Performing full fine-tuning on DistilGPT-2 and measuring training time, VRAM usage, and tuned parameters.\n",
        "*   Implementing and training DistilGPT-2 with LoRA and measuring training time, VRAM usage, and tuned parameters.\n",
        "*   Generating text samples from both the full fine-tuned and LoRA DistilGPT-2 models for qualitative comparison.\n",
        "\n",
        "We were unable to complete the evaluation metrics calculation due to a runtime error during the evaluation step.\n",
        "\n",
        "The optional experiments with GPT-2 Medium, including implementing LoRA and QLoRA, could not be completed due to environmental limitations (lack of a CUDA-enabled GPU required for `bitsandbytes`).\n",
        "\n",
        "Based on the completed DistilGPT-2 experiments, we observed that LoRA significantly reduced the number of trainable parameters and training time compared to full fine-tuning, with comparable peak VRAM usage in this run.\n",
        "\n",
        "This concludes the task based on the feasible experiments in this environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2de66286"
      },
      "source": [
        "## Summary of DistilGPT-2 Experiment Results\n",
        "\n",
        "Here is a table summarizing the key metrics from the DistilGPT-2 experiments:\n",
        "\n",
        "| Metric                        | Full Fine-tuning   | LoRA        |\n",
        "| :---------------------------- | :----------------- | :---------- |\n",
        "| Training Time (seconds)       | {{training_time:.2f}}      | {{training_time_lora:.2f}}   |\n",
        "| Peak VRAM Usage (MB)          | {{peak_vram:.2f}}      | {{peak_lora_vram:.2f}}    |\n",
        "| Number of Tuned Parameters    | {{num_tuned_params}}     | 405,504     |\n",
        "\n",
        "*Note: Evaluation metrics (Perplexity and ROUGE) could not be computed due to a runtime error during the evaluation step.*"
      ]
    }
  ]
}